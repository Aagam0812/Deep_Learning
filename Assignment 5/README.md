# Deep Learning from Scratch with Various Frameworks üöÄ

This repository is a comprehensive guide to building deep neural networks from scratch using different frameworks such as NumPy, PyTorch, TensorFlow, and JAX. Each example demonstrates the construction of a 3-layer neural network for non-linear regression, illustrating the principles of manual backpropagation, gradient propagation, and the use of non-linear activation functions.

## Description üìñ

The projects within this repository are designed to showcase the flexibility and power of manual neural network construction without relying on high-level API functionalities. Here's what's covered:

- **NumPy Neural Network**: A simple 3-layer neural network for non-linear regression built entirely with NumPy, including manual calculations for forward and backward passes.
  
  **Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Aagam0812/Deep_Learning/blob/main/Assignment%202/CMPE258_Assignment5_PartA.ipynb)


- **PyTorch from Scratch**: Implementing a 3-layer neural network using PyTorch's tensor operations without the convenience of its high-level neural network modules.

  **Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Aagam0812/Deep_Learning/blob/main/Assignment%202/CMPE258_Assignment5_PartB.ipynb)

- **PyTorch with Classes**: Leveraging PyTorch's class-based system to structure a 3-layer neural network, utilizing its built-in functionality for modules and backpropagation.

  **Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Aagam0812/Deep_Learning/blob/main/Assignment%202/CMPE258_Assignment5_PartC.ipynb)

- **PyTorch Lightning Version**: Simplifying PyTorch code further using the PyTorch Lightning framework for a cleaner and more abstracted implementation.

  **Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Aagam0812/Deep_Learning/blob/main/Assignment%202/CMPE258_Assignment5_PartD.ipynb)

- **TensorFlow Variants**:
  - **Low-level TensorFlow Implementation**: Dive into the granular control over the network's construction and training loop, offering a deep understanding of the foundational mechanics of TensorFlow operations.

  - **Functional API Usage in TensorFlow**: Leverage the Functional API for a more modular and flexible approach to constructing neural networks, allowing for complex architectures through a simple and intuitive interface.

  - **TensorFlow with Built-in Layers**: Accelerate development by utilizing TensorFlow's predefined layers, showcasing the ease and efficiency of rapid model prototyping and deployment.

  - **High-level API in TensorFlow**: Take advantage of TensorFlow's high-level API for streamlined model development and training, abstracting away the complexity of manual computations and significantly reducing the codebase.

  **Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Aagam0812/Deep_Learning/blob/main/Assignment%202/CMPE258_Assignment5_PartE.ipynb)

- **JAX Implementation**: Utilizing JAX for high-performance, GPU-accelerated numerical computing in constructing and training the neural network.

  **Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Aagam0812/Deep_Learning/blob/main/Assignment%202/CMPE258_Assignment5_PartH.ipynb)

## Features üåü

- üß† **Deep Neural Networks**: Dive into the architecture and training of deep neural networks for regression tasks.
- üìö **Manual Backpropagation**: Understand the inner workings of neural networks by manually implementing backpropagation algorithms.
- üîß **Custom Layers with `einsum`**: Use Einstein summation convention (`einsum`) for efficient tensor operations and custom layer implementations.
- üìâ **Training Visualization**: Visualize the training process with plots of losses over epochs and comparisons of actual vs. predicted outputs.

## Demo Video üìπ

[Video link](https://drive.google.com/drive/folders/1sEeoAYSQOm1SYDgyYvBuBbB8rbCHA290?usp=sharing)

## Conclusion and Insights üìù

This repository offers a hands-on approach to understanding the nuts and bolts of neural network training and architecture without the abstraction layers introduced by high-level APIs. By building these networks from scratch, you gain a deeper understanding of the foundational principles of deep learning.
